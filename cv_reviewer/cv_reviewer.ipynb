{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db0bc5a6-669c-4de4-9c32-17aef875d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils.cv_reviewer_utils as cv_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3430549e-7dd9-4de9-8d28-ea5163613c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if its published somewhere\n",
    "#job_description = get_job_description(\"url\")\n",
    "\n",
    "job_description = \"\"\"\n",
    "\n",
    "Job Title: Junior Data Engineer\n",
    "Location: Remote\n",
    "Job Type: Full-time\n",
    "Team: Data Engineering\n",
    "Reports to: Senior Data Engineer\n",
    "\n",
    "About the Role:\n",
    "\n",
    "We are looking for a Junior Data Engineer to join our growing data team. You will support the design, development, and maintenance of data pipelines and infrastructure that power data analytics, reporting, and data-driven decision-making across the company.\n",
    "This is a great opportunity for someone early in their career to learn and grow while working on real-world data challenges in a collaborative and agile environment.\n",
    "\n",
    "Key Responsibilities\n",
    "- Assist in building, maintaining, and optimizing ETL/ELT pipelines.\n",
    "- Work closely with data analysts, scientists, and engineers to support data ingestion and transformation workflows.\n",
    "- Help ensure the reliability and quality of data across systems.\n",
    "- Monitor and troubleshoot data pipelines and processes.\n",
    "- Contribute to documentation of data flows, models, and architecture.\n",
    "- Learn and apply best practices in data engineering, including security and scalability.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "- Basic knowledge of SQL and at least one programming language (Python preferred).\n",
    "- Familiarity with data storage systems (e.g., relational databases, cloud storage).\n",
    "- Understanding of data processing concepts and tools.\n",
    "- Willingness to learn and grow in a fast-paced environment.\n",
    "- Good communication and problem-solving skills.\n",
    "- Bachelor’s degree in Computer Science, Engineering, Mathematics, or related field (or equivalent practical experience).\n",
    "\n",
    "Nice to Have:\n",
    "\n",
    "- Exposure to cloud platforms like AWS, GCP, or Azure.\n",
    "- Experience with version control (e.g., Git).\n",
    "- Basic knowledge of data modeling concepts.\n",
    "- Familiarity with tools like Airflow, dbt, or Spark.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# in lower case\n",
    "mandatory_keywrods = [\"spark\",\"python\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31c8a86d-f790-44da-9a1e-3e3e1862d945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Using gpt-4o-mini via OpenAI API...\n",
      "▶️ Using gpt-4o-mini via OpenAI API...\n",
      "▶️ Using gpt-4o-mini via OpenAI API...\n",
      "▶️ Using gpt-4o-mini via OpenAI API...\n"
     ]
    }
   ],
   "source": [
    "landing_path = './landing'\n",
    "\n",
    "descriptions_dict = {}\n",
    "evaluation_dict = {}\n",
    "matches = []\n",
    "\n",
    "for filename in os.listdir(landing_path):\n",
    "    path = os.path.join(landing_path, filename)\n",
    "    \n",
    "    if os.path.isfile(path):\n",
    "        cv_text = cv_utils.extract_text_from_cv(path)\n",
    "        if cv_text is not None:\n",
    "            \n",
    "            words = cv_text.split()    \n",
    "            num_of_words = len(words)\n",
    "\n",
    "            if num_of_words > 5:    \n",
    "                descriptions_dict[filename] = cv_text\n",
    "\n",
    "for filename,candidate_desc in descriptions_dict.items():\n",
    "\n",
    "        # \"llama3.2\" or \"gpt-4o-mini\" by now :)\n",
    "        model = \"gpt-4o-mini\"\n",
    "\n",
    "        # keywords string is not mandatory\n",
    "        keywords_string = \"Additional note: \" + cv_utils.evaluate_mandatory_keywords(candidate_desc,mandatory_keywrods)\n",
    "    \n",
    "        llm_answer = cv_utils.evaluate_candidate(model, candidate_desc, job_description, keywords_string)\n",
    "        matches.append(llm_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7938c4f-47d6-4e6f-958c-03f0e3d1621c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate nº1: Thaddeus Drake - 90% → Thaddeus has robust experience in building, maintaining, and optimizing ETL pipelines using tools like Apache NiFi and Airflow, which aligns well with the role's requirements. His proficiency in SQL and Python, along with hands-on experience in cloud services like AWS, positions him as a strong candidate. He has demonstrated skills in data quality and reliability through his work with large datasets. Although he does not mention experience with Spark or GCP/Azure, his extensive background in data engineering effectively supports the job's expectations. \n",
      "\n",
      "candidate nº2: ALAN SUSA - 85% → Alan has extensive experience as a Data Engineer, with skills in SQL and Python, and has worked with various data storage systems such as Redshift and S3. He has built and maintained ETL pipelines and has experience in cloud platforms like AWS. However, as he has significant work experience, he may not align with the junior level position as closely, which typically seeks less experienced candidates. \n",
      "\n",
      "candidate nº3: John Smith - 85% → John has extensive experience as a Data Engineer with strong skills in SQL, Python, and Spark, making him well-suited for building and optimizing ETL/ELT pipelines. He has worked on large datasets and has familiarity with data storage systems and quality assurance. However, as a more senior candidate with 5+ years of experience, he may have a steeper learning curve in a Junior role compared to other entry-level applicants. \n",
      "\n",
      "candidate nº4: Elara Quinn - 70% → Elara has practical experience as a Data Engineer Intern at HP Inc., where she built pipelines and created dashboards using tools relevant to the role. She has a strong background in SQL and experience with Apache NiFi, Tableau, Amazon Redshift, and AWS, aligning well with the requirements. However, she lacks experience with Python, a specific programming language preferred for the position, and additional tools like Airflow or dbt, which are nice to have. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_matches = sorted(matches, key=lambda x: x['match_percentage'], reverse=True)\n",
    "\n",
    "n = 1\n",
    "for match in sorted_matches:\n",
    "    print(f\"candidate nº{n}: {match['name']} - {match['match_percentage']}% → {match['summary']} \\n\")\n",
    "    n+=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
