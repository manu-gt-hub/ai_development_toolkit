{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5326a-99f5-4524-ac49-9b587c9a1328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db0bc5a6-669c-4de4-9c32-17aef875d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import ollama\n",
    "from IPython.display import Markdown, display\n",
    "import pdfplumber\n",
    "from docx import Document\n",
    "import pprint\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "from dotenv import load_dotenv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2001fbe-e69e-4e5e-89e1-566f972c579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_llama3_ollama(prompt: str, model: str, system_prompt: str) -> str:\n",
    "    import ollama\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    llm_response = response[\"message\"][\"content\"]\n",
    "    return llm_response\n",
    "\n",
    "def call_chatgpt_openai(prompt: str, model_name: str, system_prompt: str) -> str:\n",
    "    load_dotenv()\n",
    "    os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "    openai = OpenAI(http_client=httpx.Client(verify=False))\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f23468-20ea-407e-ac85-cc8e2cf6f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Función para leer el contenido de un archivo .docx\n",
    "def read_docx(cv_path):\n",
    "    try:\n",
    "        # Cargar el documento\n",
    "        doc = Document(cv_path)\n",
    "        \n",
    "        # Leer el contenido del documento\n",
    "        contenido = []\n",
    "        for parrafo in doc.paragraphs:\n",
    "            contenido.append(parrafo.text)\n",
    "        \n",
    "        return '\\n'.join(contenido)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def read_doc(cv_path):\n",
    "    try:\n",
    "        # Inicializar la aplicación de Word\n",
    "        word = win32com.client.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        \n",
    "        # Abrir el documento\n",
    "        doc = word.Documents.Open(cv_path)\n",
    "        \n",
    "        # Leer el contenido del documento\n",
    "        contenido = doc.Content.Text\n",
    "        \n",
    "        # Cerrar el documento y la aplicación de Word\n",
    "        doc.Close(False)\n",
    "        word.Quit()\n",
    "        \n",
    "        return contenido\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def read_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text()\n",
    "        return text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_text_from_cv(cv_path):\n",
    "    cv_text = read_pdf(cv_path)\n",
    "    if cv_text is None:\n",
    "        cv_text = read_docx(cv_path)\n",
    "        if cv_text is None:\n",
    "            cv_text =read_doc(cv_path)\n",
    "\n",
    "    return cv_text\n",
    "\n",
    "\n",
    "def get_job_description(url):\n",
    "    class_name = 'wiki-content'\n",
    "    # Hacer la solicitud HTTP\n",
    "    response = requests.get(url, verify = False)\n",
    "    \n",
    "    # Verificar que la solicitud fue exitosa\n",
    "    if response.status_code == 200:\n",
    "        # Analizar el contenido HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Encontrar todos los elementos con la clase especificada\n",
    "        elements = soup.find_all(class_=class_name)\n",
    "        \n",
    "        # Extraer el texto de esos elementos\n",
    "        text = '\\n'.join([element.get_text(separator='\\n').strip() for element in elements])\n",
    "        return text\n",
    "    else:\n",
    "        return f\"Error: Unable to fetch the page. Status code: {response.status_code}\"\n",
    "\n",
    "def evaluate_candidate(model_source, candidate_desc, job_description):\n",
    "\n",
    "    system_prompt = \"you are a CV reviewer.\"\n",
    "       \n",
    "    prompt = f\"\"\"\n",
    "    Review the following CV description:\n",
    "\n",
    "    {candidate_desc}\n",
    "    \n",
    "    Evaluate how well this candidate matches the following job description:\n",
    "    \n",
    "    {job_description}\n",
    "    \n",
    "    You MUST pay attention mostly to the ROLE and the SENIORITY match between what we are looking for and the candidate's seniority, and answer ONLY with a JSON object in the following structure:\n",
    "    \n",
    "    {{\n",
    "      \"name\": \"Candidate Name\",\n",
    "      \"match_percentage\": number between 0 and 100,\n",
    "      \"summary\": \"A  summary explaining the match, including relevant skills, technologies, and gaps\"\n",
    "    }}\n",
    "    \n",
    "    ❗️Output STRICTLY as valid JSON. Do NOT include any explanations, extra text, markdown formatting, or comments.\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    {{\n",
    "      \"name\": \"John Marston\",\n",
    "      \"match_percentage\": 78,\n",
    "      \"summary\": \"ihe has strong knowledge on AWS, he has developed using Spark, Python and knows a few database systems.\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    if \"llama\" in model_source.lower():\n",
    "        print(f\"▶️ Using {model_source} via Ollama...\")\n",
    "        full_response = call_llama3_ollama(prompt, model_source, system_prompt)\n",
    "    \n",
    "    elif \"gpt\" in model_source.lower():\n",
    "        print(f\"▶️ Using {model_source} via OpenAI API...\")\n",
    "        full_response = call_chatgpt_openai(prompt, model_source, system_prompt)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_source. Use 'llama3' or 'chatgpt'.\")\n",
    "\n",
    "    #print(prompt)\n",
    "\n",
    "    # clean markdown delimiter\n",
    "    clean_response = full_response.replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "    #print(full_response)\n",
    "    \n",
    "    result = json.loads(clean_response)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3430549e-7dd9-4de9-8d28-ea5163613c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#job_description = get_job_description(\"url\")\n",
    "\n",
    "job_description = \"\"\"\n",
    "\n",
    "Job Title: Junior Data Engineer\n",
    "Location: Remote\n",
    "Job Type: Full-time\n",
    "Team: Data Engineering\n",
    "Reports to: Senior Data Engineer\n",
    "\n",
    "About the Role:\n",
    "\n",
    "We are looking for a Junior Data Engineer to join our growing data team. You will support the design, development, and maintenance of data pipelines and infrastructure that power data analytics, reporting, and data-driven decision-making across the company.\n",
    "This is a great opportunity for someone early in their career to learn and grow while working on real-world data challenges in a collaborative and agile environment.\n",
    "\n",
    "Key Responsibilities\n",
    "- Assist in building, maintaining, and optimizing ETL/ELT pipelines.\n",
    "- Work closely with data analysts, scientists, and engineers to support data ingestion and transformation workflows.\n",
    "- Help ensure the reliability and quality of data across systems.\n",
    "- Monitor and troubleshoot data pipelines and processes.\n",
    "- Contribute to documentation of data flows, models, and architecture.\n",
    "- Learn and apply best practices in data engineering, including security and scalability.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "- Basic knowledge of SQL and at least one programming language (Python preferred).\n",
    "- Familiarity with data storage systems (e.g., relational databases, cloud storage).\n",
    "- Understanding of data processing concepts and tools.\n",
    "- Willingness to learn and grow in a fast-paced environment.\n",
    "- Good communication and problem-solving skills.\n",
    "- Bachelor’s degree in Computer Science, Engineering, Mathematics, or related field (or equivalent practical experience).\n",
    "\n",
    "Nice to Have:\n",
    "\n",
    "- Exposure to cloud platforms like AWS, GCP, or Azure.\n",
    "- Experience with version control (e.g., Git).\n",
    "- Basic knowledge of data modeling concepts.\n",
    "- Familiarity with tools like Airflow, dbt, or Spark.\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31c8a86d-f790-44da-9a1e-3e3e1862d945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Using gpt-4o-mini via OpenAI API...\n",
      "▶️ Using gpt-4o-mini via OpenAI API...\n",
      "▶️ Using gpt-4o-mini via OpenAI API...\n",
      "▶️ Using gpt-4o-mini via OpenAI API...\n"
     ]
    }
   ],
   "source": [
    "directorio = './landing'\n",
    "\n",
    "descriptions_dict = {}\n",
    "evaluation_dict = {}\n",
    "matches = []\n",
    "\n",
    "for nombre_archivo in os.listdir(directorio):\n",
    "    ruta_archivo = os.path.join(directorio, nombre_archivo)\n",
    "    if os.path.isfile(ruta_archivo):\n",
    "        cv_text = extract_text_from_cv(ruta_archivo)\n",
    "        if cv_text is not None:\n",
    "            \n",
    "            words = cv_text.split()    \n",
    "            num_of_words = len(words)\n",
    "\n",
    "            if num_of_words > 5:    \n",
    "                descriptions_dict[nombre_archivo] = cv_text\n",
    "\n",
    "for filename,candidate_desc in descriptions_dict.items():\n",
    "\n",
    "        # \"llama3.2\" or \"gpt-4o-mini\"\n",
    "        model = \"gpt-4o-mini\"\n",
    "    \n",
    "        llm_answer = evaluate_candidate(model, candidate_desc, job_description)\n",
    "        matches.append(llm_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7938c4f-47d6-4e6f-958c-03f0e3d1621c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate nº1: Alan Susa - 85% → Alan has substantial experience as a Data Engineer, demonstrating strong skills in Python, SQL, and various data storage systems, including AWS services. He has built and maintained ETL pipelines and has familiarity with tools like Spark and Kafka. While his experience exceeds the junior level, he shows high potential for growth in a collaborative environment. \n",
      "\n",
      "candidate nº2: Elara Quinn - 85% → Elara has relevant experience as a Data Engineer Intern, having built data pipelines and working with SQL, Apache Hadoop, and Tableau. She demonstrates familiarity with cloud platforms and data ingestion processes, aligning well with the requirements. However, her profile lacks explicit mention of a programming language like Python and basic knowledge of data modeling concepts, which are desired for this role. \n",
      "\n",
      "candidate nº3: Thaddeus - 85% → Thaddeus has solid experience as a Junior Data Engineer, with proven skills in MySQL, Python, and various data pipelines including AWS and Apache NiFi. He has hands-on experience in ETL processes, which aligns well with the key responsibilities of the role. Gaps may include the level of familiarity with data modeling concepts and documentation practices. \n",
      "\n",
      "candidate nº4: John Smith - 70% → John has over 5 years of experience as a Data Engineer, which exceeds the junior level expectation. He is proficient in SQL and Python and has experience with ETL processes, making him suitable for building and optimizing pipelines. However, the role requires a basic level of knowledge, making him potentially overqualified. Familiarity with cloud platforms and version control is not evident in his CV, indicating a gap. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_matches = sorted(matches, key=lambda x: x['match_percentage'], reverse=True)\n",
    "\n",
    "n = 1\n",
    "for match in sorted_matches:\n",
    "    print(f\"candidate nº{n}: {match['name']} - {match['match_percentage']}% → {match['summary']} \\n\")\n",
    "    n+=1\n",
    "\n",
    "\n",
    "# Thaddeus - 85% → Matches most of the key responsibilities, skills, and technologies required by the job description\n",
    "# Alan Susa - 80% → Matches well for requirements, skills, and technologies with strong emphasis on ETL/ELT pipelines and data processing concepts. Some junior-level experience compared to the job description\n",
    "# Elara Quinn - 80% → Matches for most of the job requirements, including ETL/ELT pipelines, data ingestion, and cloud platforms like AWS. Some experience with Tableau is mentioned but not directly related to the key responsibilities\n",
    "# John Smith - 60% → Matches for basic requirements and skills with good exposure to SQL, Python, Hadoop, and Apache Spark. Lacks experience in cloud platforms, version control, and some tools mentioned in the job description"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
