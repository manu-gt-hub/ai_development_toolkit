{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552f7439-9171-4eb7-9168-1278b61f38bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISCLAIMER:\n",
    "# \n",
    "# all CVs used here were found in: https://www.beamjobs.com/resumes/data-engineer-resume-examples\n",
    "\n",
    "# CONCLUSIONS\n",
    "#\n",
    "# the key point using LLAMA3 is to retrieve the seniority from the CV (overall and specific)\n",
    "# we could use another tool to extract it and send it to the LLM already gathered to improve its performance\n",
    "# alternative: use the LLM to get every experience the candidate has and calculate and store the experience using code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78532789-97ed-48db-91ce-2cd4ad13b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import ollama\n",
    "from IPython.display import Markdown, display\n",
    "import pdfplumber\n",
    "from docx import Document\n",
    "import pprint\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "seniority = \"senior\"\n",
    "\n",
    "job_desc_text = f\"\"\"\n",
    "Position Description:\n",
    "\n",
    "We are seeking a {seniority} Data Engineer to design, build, and maintain scalable data pipelines. The ideal candidate will have expertise in SQL, ETL processes, and cloud technologies, collaborating closely with Data Scientists to ensure data integration and quality.\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "Manage cloud data storage systems.\n",
    "Collaborate with data scientists to meet data requirements.\n",
    "Ensure data quality and security.\n",
    "Automate data processes.\n",
    "Monitor and troubleshoot data systems.\n",
    "Optimize Big Data solutions.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "Education: Degree in Computer Science or related field.\n",
    "Proficiency in programming languages such as Python, Go, or Rust.\n",
    "Strong SQL skills.\n",
    "Experience with Hadoop and Kafka.\n",
    "Familiarity with cloud platforms (IBM Cloud, Oracle Cloud).\n",
    "Knowledge of data orchestration tools like Prefect or Luigi.\n",
    "Experience in CI/CD tools (GitLab CI, CircleCI).\n",
    "\n",
    "Desirable:\n",
    "\n",
    "Experience with Snowflake.\n",
    "Knowledge of visualization tools (Tableau, PowerBI).\n",
    "Familiarity with Docker or Kubernetes.\n",
    "Understanding of agile methodologies.\n",
    "Cloud or big data certifications.\n",
    "Multicultural experience.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f23468-20ea-407e-ac85-cc8e2cf6f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Función para leer el contenido de un archivo .docx\n",
    "def read_docx(cv_path):\n",
    "    try:\n",
    "        # Cargar el documento\n",
    "        doc = Document(cv_path)\n",
    "        \n",
    "        # Leer el contenido del documento\n",
    "        contenido = []\n",
    "        for parrafo in doc.paragraphs:\n",
    "            contenido.append(parrafo.text)\n",
    "        \n",
    "        return '\\n'.join(contenido)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def read_doc(cv_path):\n",
    "    try:\n",
    "        # Inicializar la aplicación de Word\n",
    "        word = win32com.client.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        \n",
    "        # Abrir el documento\n",
    "        doc = word.Documents.Open(cv_path)\n",
    "        \n",
    "        # Leer el contenido del documento\n",
    "        contenido = doc.Content.Text\n",
    "        \n",
    "        # Cerrar el documento y la aplicación de Word\n",
    "        doc.Close(False)\n",
    "        word.Quit()\n",
    "        \n",
    "        return contenido\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def read_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text()\n",
    "        return text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_text_from_cv(cv_path):\n",
    "    cv_text = read_pdf(cv_path)\n",
    "    if cv_text is None:\n",
    "        cv_text = read_docx(cv_path)\n",
    "        if cv_text is None:\n",
    "            cv_text =read_doc(cv_path)\n",
    "\n",
    "    return cv_text\n",
    "\n",
    "\n",
    "def get_job_description(url):\n",
    "    class_name = 'wiki-content'\n",
    "    # Hacer la solicitud HTTP\n",
    "    response = requests.get(url, verify = False)\n",
    "    \n",
    "    # Verificar que la solicitud fue exitosa\n",
    "    if response.status_code == 200:\n",
    "        # Analizar el contenido HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Encontrar todos los elementos con la clase especificada\n",
    "        elements = soup.find_all(class_=class_name)\n",
    "        \n",
    "        # Extraer el texto de esos elementos\n",
    "        text = '\\n'.join([element.get_text(separator='\\n').strip() for element in elements])\n",
    "        return text\n",
    "    else:\n",
    "        return f\"Error: Unable to fetch the page. Status code: {response.status_code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3430549e-7dd9-4de9-8d28-ea5163613c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "roles = [\"data engineer\",\"full stack developer\",\"machine learning engineer\",\"data scientist\"]\n",
    "seniority_dict = {\n",
    "    \"senior\" : \"more than 4 years of experience in total\",\n",
    "    \"mid-senior\" : \"between 2 and 4 years of experience in total\",\n",
    "    \"junior\" : \"between 0 and 2 years of experience in total\"\n",
    "}\n",
    "\n",
    "directorio = './resources/cvs_landing'\n",
    "\n",
    "descriptions_dict = {}\n",
    "\n",
    "for nombre_archivo in os.listdir(directorio):\n",
    "    ruta_archivo = os.path.join(directorio, nombre_archivo)\n",
    "    if os.path.isfile(ruta_archivo):\n",
    "        cv_text = extract_text_from_cv(ruta_archivo)\n",
    "        if cv_text is not None:\n",
    "            \n",
    "            words = cv_text.split()    \n",
    "            num_of_words = len(words)\n",
    "\n",
    "            if num_of_words > 5:    \n",
    "                descriptions_dict[nombre_archivo] = cv_text\n",
    "\n",
    "for filename,desc in descriptions_dict.items():\n",
    "    \n",
    "        descriptions_dict[filename] = desc\n",
    "        #print(filename+\"\\n\\n\"+desc+\"\\n-------------------------------------------------------------------------------\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ee16bc2-85e6-4cb7-8c6c-d53a48ed8b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_prompt():\n",
    "    prompt = \"\"\n",
    "    \n",
    "    prompt += f\"\"\"answer me with a list of candidates based on this dictionary: \\n\\n\"\"\"\n",
    "    \n",
    "    for fn,desc in descriptions_dict.items():\n",
    "        prompt += desc + \"\\n\\n\"\n",
    "    \n",
    "    prompt += f\"\"\"from the first interesting candidate we should interview to the less for the job description: {job_desc_text},\n",
    "            pay attention to the candidate's seniority level and the rquired for the job, also the technologies that the candidates manage. the answer has to be ready to be printed in markdown and a summarized description of all the candidates.\"\"\"\n",
    "    return prompt\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4060d846-3f9d-4dca-af0f-7318ec4f0188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Candidate Comparison Summary\n",
       "Based on the provided dictionaries, here is a summary comparison of the candidates' suitability for the senior Data Engineer position.\n",
       "\n",
       "## Seniority Level:\n",
       "1. Alan Susa - Mid-level (5+ years of experience)\n",
       "2. Brandon Connor - Junior (Internship experience)\n",
       "3. Elara Quinn - Mid-level (2+ years of experience)\n",
       "4. Marlowe - Mid-level (2+ years of experience)\n",
       "5. John Smith - Senior (5+ years of experience)\n",
       "6. Thaddeus Drake - Mid-level (1+ year of experience)\n",
       "\n",
       "## Required Technologies:\n",
       "1. Alan Susa: Amazon Athena, S3, Kafka, PySpark\n",
       "2. Brandon Connor: Python, SQL, Spark\n",
       "3. Elara Quinn: Apache Hadoop, MySQL, Tableau, AWS\n",
       "4. Marlowe: Airflow, Presto, Hive\n",
       "5. John Smith: Machine learning tools (TensorFlow, Keras), SQL, Java\n",
       "6. Thaddeus Drake: MySQL, Apache NiFi, Snowflake, AWS\n",
       "\n",
       "## Ranking from Most Suitable to Least:\n",
       "1. John Smith - Senior level with 5+ years of experience and expertise in machine learning tools.\n",
       "2. Alan Susa - Mid-level with 5+ years of experience and expertise in cloud technologies (Athena, S3, Kafka).\n",
       "3. Marlowe - Mid-level with 2+ years of experience and expertise in data orchestration tools (Airflow).\n",
       "4. Elara Quinn - Mid-level with 2+ years of experience and expertise in Big Data solutions (Hadoop, MySQL, Tableau).\n",
       "5. Thaddeus Drake - Mid-level with 1+ year of experience and expertise in cloud technologies (AWS, Snowflake).\n",
       "6. Brandon Connor - Junior with internship experience.\n",
       "\n",
       "Note: The ranking is based on the candidate's seniority level, required technologies, and overall fit for the position."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LLAMA3 TEST\n",
    "\n",
    "response = ollama.chat(\n",
    "model=\"llama3.2\",\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": generate_prompt()\n",
    "    }\n",
    "],\n",
    ")\n",
    "llm_response = response[\"message\"][\"content\"]\n",
    "display(Markdown(llm_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "009e886d-f30d-405d-87f4-86ef8672f7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```markdown\n",
      "## Candidate Ranking for Senior Data Engineer Position\n",
      "\n",
      "1. **ALAN SUSA**\n",
      "   - **Title**: Data Engineer\n",
      "   - **Location**: New York, NY\n",
      "   - **Experience**: Over 9 years in data engineering, with expertise in building scalable data pipelines using Kafka, PySpark, and AWS technologies (Athena, Lambda, S3). Previously migrated data solutions resulting in significant cost savings and performance improvements. Extensive experience in ETL and cloud data storage management.\n",
      "   - **Education**: B.A. in Computer Science from University of Pittsburgh.\n",
      "   - **Skills**: Python, SQL (Postgres, Redshift, MySQL), NoSQL (MongoDB), Spark, Kafka, Airflow, AWS.\n",
      "\n",
      "2. **JOHN SMITH**\n",
      "   - **Title**: Data Engineer\n",
      "   - **Location**: Los Angeles, CA\n",
      "   - **Experience**: 5+ years as a Data Engineer, strong in SQL and machine learning models within banking environments. Developed ETL processes and scalable databases, increasing data processing efficiency. Experience in collaborating with data scientists on quantitative queries and machine-readable databases.\n",
      "   - **Education**: BS in Computer Science from Texas University.\n",
      "   - **Skills**: SQL, Java, Apache Spark, Hadoop, Python.\n",
      "\n",
      "3. **IANTHE MARLOWE**\n",
      "   - **Title**: Data Engineer\n",
      "   - **Location**: Austin, TX\n",
      "   - **Experience**: Currently a data engineer at Facebook with experience in optimizing data pipelines and integrating cloud solutions. Skilled in managing large datasets and improving query performance.\n",
      "   - **Education**: Bachelor of Science in Computer Science from Carnegie Mellon University.\n",
      "   - **Skills**: Presto, Apache Hive, Apache Airflow, Amazon S3, Apache Kafka, MySQL, Tableau.\n",
      "\n",
      "4. **THADDEUS DRAKE**\n",
      "   - **Title**: Junior Data Engineer\n",
      "   - **Location**: San Diego, CA\n",
      "   - **Experience**: Developed and optimized MySQL queries and automated ETL processes for genomic data. Deployed pipelines on AWS, indicating familiarity with cloud technology. Experienced in using Apache NiFi and Airflow.\n",
      "   - **Education**: Bachelor of Science in Computer Science from University of California.\n",
      "   - **Skills**: MySQL, Apache NiFi, AWS, Apache Airflow, Apache Hadoop, Python.\n",
      "\n",
      "5. **BRANDON CONNOR**\n",
      "   - **Title**: Data Engineering Intern\n",
      "   - **Location**: Austin, TX\n",
      "   - **Experience**: Intern experience focusing on cloud-first data ingestion and collaborating on BI tools. Reduced processing time and improved data utilization for e-commerce platforms.\n",
      "   - **Education**: B.S. in Computer Science from University of Texas.\n",
      "   - **Skills**: Python, SQL, ETLs, APIs, Spark, AWS (Redshift).\n",
      "\n",
      "6. **ELARA QUINN**\n",
      "   - **Title**: Data Engineer Intern\n",
      "   - **Location**: Austin, TX\n",
      "   - **Experience**: Internship at HP Inc. involving the creation of data pipelines and analytics projects. Familiar with MySQL, Apache NiFi, and Tableau. Still early in career progression with limited experience.\n",
      "   - **Education**: Bachelor of Science in Computer Science (current) from University of Texas.\n",
      "   - **Skills**: MySQL, Apache NiFi, Amazon Redshift, Apache Hadoop, AWS, Tableau.\n",
      "\n",
      "```\n",
      "\n",
      "### Summary\n",
      "The candidates are ranked primarily by their years of relevant experience, technical expertise aligning with the position's requirements (e.g., proficiency in SQL, ETL processes, and cloud technology), and the ability to manage and collaborate closely with Data Scientists. Alan Susa stands out as the strongest candidate given his seniority, comprehensive experience, and successful history of working with big data technologies.\n"
     ]
    }
   ],
   "source": [
    "# GPT 4-o-mini TEST\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "openai = OpenAI(http_client=httpx.Client(verify=False))\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a candidates classifier, I'm going to provide you some candidates description and you are going to order them from most interesting to interview to the low.\"},\n",
    "        {\"role\": \"user\", \"content\": generate_prompt()}\n",
    "    ]\n",
    ")\n",
    "\n",
    "gpt_answer = response.choices[0].message.content\n",
    "print(gpt_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
