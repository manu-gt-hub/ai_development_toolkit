{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a526efa-cda2-4a8f-bc1e-5e6a6676d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "\n",
    "def run_python_script(script_path: str):\n",
    "    \"\"\"\n",
    "    Executes a Python script given its path.\n",
    "\n",
    "    Args:\n",
    "        script_path (str): Path to the .py file to be executed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run([\"python\", script_path], capture_output=True, text=True, check=True)\n",
    "        if \"An error occurred\" in result.stdout:\n",
    "            print(\"‚ö†Ô∏è The script finished with internal errors:\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(\"‚úÖ Script executed successfully.\")\n",
    "            print(result.stdout)\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"‚ùå Error while executing the script:\")\n",
    "        print(e.stderr)\n",
    "\n",
    "\n",
    "def find_first_file(directory: str, extension: str) -> str:\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(extension):\n",
    "            return os.path.join(directory, filename)\n",
    "    raise FileNotFoundError(f\"No .py files found in directory: {directory}\")\n",
    "\n",
    "def call_llama3_ollama(prompt: str, model: str, system_prompt: str) -> str:\n",
    "    import ollama\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    llm_response = response[\"message\"][\"content\"]\n",
    "    return llm_response\n",
    "\n",
    "def call_chatgpt_openai(prompt: str, model_name: str, system_prompt: str) -> str:\n",
    "    load_dotenv()\n",
    "    os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "    openai = OpenAI(http_client=httpx.Client(verify=False))\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def optimize_script(input_path: str, extension: str, output_dir: str, model_source: str, system_prompt: str, output_file_name=\"optimized_script.py\"):\n",
    "    if not os.path.isdir(input_path):\n",
    "        raise NotADirectoryError(f\"{input_path} is not a directory\")\n",
    "\n",
    "    file_path = find_first_file(input_path, extension)\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        original_code = f.read()\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a senior Python engineer.\n",
    "    \n",
    "    Refactor the following Python code with the following goals:\n",
    "    \n",
    "    1. Add clear and concise comments explaining each function, its parameters, return values, and complex logic.\n",
    "    2. Document all functions using standard Python docstrings (PEP 257).\n",
    "    3. Remove unused imports and dependencies.\n",
    "    4. Simplify repetitive or redundant code by creating reusable functions.\n",
    "    5. Use descriptive variable, function, and class names.\n",
    "    6. Eliminate dead or unused code.\n",
    "    7. Add try/except blocks for error handling where appropriate.\n",
    "    8. Optimize loops and pandas/numpy operations (prefer vectorized solutions).\n",
    "    9. Separate logic into functions or classes.\n",
    "    10. Write basic unit tests for each function.\n",
    "    11. Ensure PEP 8 compliance.\n",
    "    12. write unit tests\n",
    "    \n",
    "    IMPORTANT: Return ONLY the improved Python script and a section NOTE: with the explanation about what you have done IN PLAIN TEXT\n",
    "    \n",
    "    ```python\n",
    "    {original_code}\n",
    "    ```\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    if \"llama\" in model_source.lower():\n",
    "        print(f\"‚ñ∂Ô∏è Using {model_source} via Ollama...\")\n",
    "        full_response = call_llama3_ollama(prompt, model_source, system_prompt)\n",
    "    \n",
    "    elif \"gpt\" in model_source.lower():\n",
    "        print(f\"‚ñ∂Ô∏è Using {model_source} via OpenAI API...\")\n",
    "        full_response = call_chatgpt_openai(prompt, model_source, system_prompt)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_source. Use 'llama3' or 'chatgpt'.\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "    # clean markdown delimiter\n",
    "    full_response = full_response.replace(\"```python\", \"\").strip()\n",
    "    parts = parts = full_response.split(\"```\")\n",
    "\n",
    "    # üî∏ here is the optimized clean code\n",
    "    optimized_code = parts[0].replace(\"```\", \"\").strip()\n",
    "    \n",
    "    # üî∏ here is the NOTE section\n",
    "    if len(parts) > 1:\n",
    "        notes = parts[1].replace(\"```\", \"\").strip()\n",
    "        print(\"üìù Notes from LLM:\\n\", notes)\n",
    "    else:\n",
    "        notes = None\n",
    "        print(\"‚ö†Ô∏è No notes section found.\")\n",
    "\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(optimized_code)\n",
    "\n",
    "    print(f\"‚úÖ Optimized script saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1742169f-9648-4d42-9e1e-2aaaafaaa44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./resources/original_script/original_notebook.py\n",
      "‚ñ∂Ô∏è Using gpt-4o-mini via OpenAI API...\n",
      "üìù Notes from LLM:\n",
      " NOTE: \n",
      "1. The code had redundant imports which were removed. \n",
      "2. A clear and concise function `preprocess_data` was created and documented with a docstring according to PEP 257. \n",
      "3. Try/except blocks were added to handle potential errors during processing. \n",
      "4. Variable names were made more descriptive.\n",
      "5. All processing logic was retained and structured simply within a single preprocessing function for clarity.\n",
      "6. Basic unit tests for the processing function are included in a conditional block for testing. \n",
      "7. The code adheres to PEP 8 standards for formatting and style.\n",
      "‚úÖ Optimized script saved to: ./resources/optimized_script/optimized_gpt.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "system_prompt = \"you are a senior python developer.\"\n",
    "\n",
    "input_path = \"./resources/original_script/\"\n",
    "output_path = \"./resources/optimized_script/\"\n",
    "\n",
    "extension = \".py\"\n",
    "\n",
    "# \"llama3.2\" or \"gpt-4o-mini\"\n",
    "model = \"gpt-4o-mini\"\n",
    "output_file_name = \"optimized_gpt.py\"\n",
    "\n",
    "# call optimizer\n",
    "optimize_script(input_path, extension, output_path, model, system_prompt, output_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6cce491-63dc-4b72-8f35-b4dd3446a35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Script ejecutado con √©xito.\n",
      "Empty DataFrame\n",
      "Columns: [date, close, daily_return, cumulative_return, ma_20, ma_50, volatility_20, ma_signal, ma_crossover, ma_crossover_signal, trend]\n",
      "Index: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# script path\n",
    "script_path = output_path + output_file_name\n",
    "\n",
    "# run the optimized script to ensure that the code runs OK\n",
    "run_python_script(script_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
